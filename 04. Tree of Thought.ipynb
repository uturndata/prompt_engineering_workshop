{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1952dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4867a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks up the id for the guardrail named 'sanitize'\n",
    "bedrock_client = boto3.client('bedrock')\n",
    "guardrails = bedrock_client.list_guardrails()\n",
    "this_guardrail_id = None\n",
    "for guardrail in guardrails['guardrails']:\n",
    "    if guardrail['name'] == 'sanitizer':\n",
    "        this_guardrail_id = guardrail['id']\n",
    "        break\n",
    "print(\"guardrail ID: \" + this_guardrail_id)\n",
    "\n",
    "# reference to the foundation model via amazon bedrock.\n",
    "# https://docs.aws.amazon.com/bedrock/latest/userguide/inference-parameters.html\n",
    "model = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",  # claude is awesome!    \n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.95,\n",
    "        # \"top_p\": 0.95,\n",
    "        # \"top_k\": 250,\n",
    "        \"max_tokens\": 20000\n",
    "    },\n",
    "    guardrails={                                         # guardrails filter prompts and responses\n",
    "        \"guardrailIdentifier\": this_guardrail_id, \n",
    "        \"guardrailVersion\": \"1\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba705283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are no experts!\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "Imagine three different experts are answering this question.\n",
    "\n",
    "All experts will write down 1 step of their thinking,\n",
    "then share it with the group.\n",
    "\n",
    "Then all experts will go on to the next step, etc.\n",
    "\n",
    "If any expert realises they're wrong at any point then they leave.\n",
    "\n",
    "The question is... what is 2*13-23*1(2%2)?\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47178422",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4fb4bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(chain.invoke({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7289f9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
